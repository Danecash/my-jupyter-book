{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac05ac7d",
   "metadata": {},
   "source": [
    "# **Laboratory Activty**\n",
    "\n",
    "`Instruction:` Convert the following CNN architecture diagram into a PyTorch CNN Architecture.\n",
    "\n",
    "![image](quick_draw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad4e55e",
   "metadata": {},
   "source": [
    "**Step 1:** Input\n",
    "\n",
    "- Our input image is grayscale with size 28 × 28.\n",
    "\n",
    "- Shape in PyTorch: [batch_size, channels, height, width].\n",
    "\n",
    "    - Example: for 1 image → [1, 1, 28, 28]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a809a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(1, 1, 28, 28)  # batch=1, channel=1, H=28, W=28\n",
    "print(\"Input shape:\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd26f1",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "This is the starting point of the CNN. We will now transform this input step by step using convolution and pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2d330",
   "metadata": {},
   "source": [
    "**Step 2: First Convolution + Pooling (Conv1 + MaxPool1)**\n",
    "\n",
    "- **Conv1:** kernel=3×3, stride=1, padding=1 → preserves size (28 → 28).\n",
    "\n",
    "- Assume **32 filters** (output channels).\n",
    "\n",
    "- **MaxPool1:** kernel=2×2, stride=2, padding=1 → reduces size but padding keeps it larger than usual (28 → 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f752e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Conv1 + Pool1: torch.Size([1, 32, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "\n",
    "out1 = F.relu(conv1(x))\n",
    "out1 = pool1(out1)\n",
    "print(\"After Conv1 + Pool1:\", out1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da876862",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The convolution expanded the depth to 32 (learned filters).\n",
    "\n",
    "- Pooling with padding shrunk the image to 15×15 (instead of 14×14)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca3152",
   "metadata": {},
   "source": [
    "**Step 3: Deeper Convolutions (Conv2, Conv3, Conv4)**\n",
    "\n",
    "- Each convolution: kernel=3×3, stride=1, padding=1 → preserves size (15×15).\n",
    "\n",
    "- Channels: 32 → 64 → 128 → 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b195a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Conv4: torch.Size([1, 128, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "out2 = F.relu(conv2(out1))\n",
    "out3 = F.relu(conv3(out2))\n",
    "out4 = F.relu(conv4(out3))\n",
    "print(\"After Conv4:\", out4.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a2a33",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The network learned more complex features by stacking multiple convolution layers.\n",
    "\n",
    "- Size stayed the same (15×15), but depth increased to 128."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ade7a3",
   "metadata": {},
   "source": [
    "**Step 4: Second Pooling (MaxPool2)**\n",
    "\n",
    "- Kernel=2×2, stride=2, padding=0 → halves the size (15 → 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4ce380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Pool2: torch.Size([1, 128, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "out5 = pool2(out4)\n",
    "print(\"After Pool2:\", out5.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d33f13",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "Pooling reduces spatial size to focus on essential features while discarding redundant details.\n",
    "\n",
    "Now we have a compact representation: 128 filters of size 7×7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d074a40",
   "metadata": {},
   "source": [
    "**Step 5: Dropout + Flatten**\n",
    "\n",
    "- Dropout randomly sets 20% of values to zero → prevents overfitting.\n",
    "\n",
    "- Flatten converts `[1, 128, 7, 7]` → `[1, 6272]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93256d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Dropout + Flatten: torch.Size([1, 6272])\n"
     ]
    }
   ],
   "source": [
    "dropout = nn.Dropout(p=0.2)\n",
    "out6 = dropout(out5)\n",
    "out7 = torch.flatten(out6, start_dim=1)\n",
    "print(\"After Dropout + Flatten:\", out7.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572825dc",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Dropout makes the model more robust.\n",
    "\n",
    "- Flatten prepares the tensor for fully connected layers.\n",
    "\n",
    "- `6272` = `128 × 7 × 7`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7798488",
   "metadata": {},
   "source": [
    "**Step 6: Fully Connected Layers**\n",
    "\n",
    "From diagram:\n",
    "\n",
    "1. FC1: input=6272, output=1000\n",
    "\n",
    "2. FC2: input=1000, output=500                        \n",
    "\n",
    "3. FC3: input=500, output=10 (number of classes, e.g., MNIST digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc4b55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "fc1 = nn.Linear(6272, 1000)\n",
    "fc2 = nn.Linear(1000, 500)\n",
    "fc3 = nn.Linear(500, 10)\n",
    "\n",
    "out8 = F.relu(fc1(out7))\n",
    "out9 = F.relu(fc2(out8))\n",
    "logits = fc3(out9)\n",
    "print(\"Final output:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b30f8",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The final vector of length 10 represents class scores (logits).\n",
    "\n",
    "- For MNIST, each value corresponds to digits 0–9.\n",
    "\n",
    "- We use `CrossEntropyLoss` for training (applies Softmax internally)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14729d3",
   "metadata": {},
   "source": [
    "**Step 7: Wrap Everything into a Class**\n",
    "\n",
    "Now we package the whole network into a reusable PyTorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ba02a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class DiagramCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, 1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2, 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2, 0)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(6272, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        self.fc3 = nn.Linear(500, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Test model\n",
    "model = DiagramCNN()\n",
    "print(model(torch.randn(1, 1, 28, 28)).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36b2a6",
   "metadata": {},
   "source": [
    "**Final Summary**\n",
    "\n",
    "- Input image (28×28) passed through two **convolution–pooling blocks** and **dropout**.\n",
    "\n",
    "- Final flatten size = 6272, fed into 3 fully connected layers.\n",
    "\n",
    "- Output = `[batch, 10]` → classification scores for 10 classes.\n",
    "\n",
    "- This CNN follows the given diagram exactly and is ready for training with `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99659aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
