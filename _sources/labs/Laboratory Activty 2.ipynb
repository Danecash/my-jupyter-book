{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eeba8d8",
   "metadata": {},
   "source": [
    "# Laboratory Activty 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e2639",
   "metadata": {},
   "source": [
    "### Laboratory Task 2\n",
    "![image](./images/lab2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44463186",
   "metadata": {},
   "source": [
    "### **Define Inputs and Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82936fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define inputs and weights (as given in the problem)\n",
      "\n",
      "x = [1 0 1]\n",
      "y = 1\n",
      "W_hidden =\n",
      " [[ 0.2 -0.3]\n",
      " [ 0.4  0.1]\n",
      " [-0.5  0.2]]\n",
      "theta (output parameters) = [-0.4  0.2  0.1]\n"
     ]
    }
   ],
   "source": [
    "# Define inputs and weights (given values)\n",
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "print(\"Define inputs and weights (as given in the problem)\\n\")\n",
    "\n",
    "# Input and target\n",
    "x = np.array([1, 0, 1])        # input vector (3,)\n",
    "y = 1                          # desired target (scalar)\n",
    "\n",
    "# Hidden-unit weight matrix (3 inputs -> 2 hidden units)\n",
    "# Interpreting the matrix as rows = inputs, cols = hidden units:\n",
    "#   [[w11, w12],\n",
    "#    [w13, w14],\n",
    "#    [w15, w16]]\n",
    "W_hidden = np.array([\n",
    "    [0.2, -0.3],\n",
    "    [0.4,  0.1],\n",
    "    [-0.5, 0.2]\n",
    "])\n",
    "\n",
    "# Output-layer parameters (interpreting θ = [bias, w_h1, w_h2])\n",
    "theta = np.array([-0.4, 0.2, 0.1])\n",
    "\n",
    "# The problem statement also listed \"output unit weights = [w21=-0.3, w22=-0.2]\".\n",
    "# To avoid ambiguity we will use the provided θ vector for the final output computation\n",
    "# (θ = [bias, weight_hidden1, weight_hidden2]) because it clearly includes a bias term.\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"W_hidden =\\n\", W_hidden)\n",
    "print(\"theta (output parameters) =\", theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4408f4c",
   "metadata": {},
   "source": [
    "In this step, we define all the given values from the problem:\n",
    "\n",
    "- **x** represents the input vector `[1, 0, 1]`.  \n",
    "- **y** is the target output, which equals `1`.  \n",
    "- **W_hidden** is the weight matrix connecting the input layer to the hidden layer. It has two hidden neurons.  \n",
    "- **θ (theta)** contains the bias and weights for the output unit, represented as `[bias, w_h1, w_h2] = [-0.4, 0.2, 0.1]`.\n",
    "\n",
    "These parameters will be used throughout the forward pass to calculate activations and final output.  \n",
    "They define how information flows from inputs through the hidden layer to the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4401b0b",
   "metadata": {},
   "source": [
    "### **Hidden Layer Pre-activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb86128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer pre-activation (z_hidden)\n",
      "\n",
      "z_hidden = [-0.3 -0.1]\n"
     ]
    }
   ],
   "source": [
    "# Compute hidden layer pre-activation (z_hidden)\n",
    "print(\"Hidden layer pre-activation (z_hidden)\\n\")\n",
    "# z_hidden_j = sum_i x_i * w_ij  --> using x dot W_hidden  (x shape (3,), W_hidden shape (3,2))\n",
    "z_hidden = x.dot(W_hidden)   # results in shape (2,)\n",
    "print(\"z_hidden =\", z_hidden)   # expected [-0.3, -0.1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055bb10e",
   "metadata": {},
   "source": [
    "Here, we compute the **pre-activation values (z_hidden)** for each hidden neuron using the equation:\n",
    "\n",
    "\\[\n",
    "z_{hidden} = x \\cdot W_{hidden}\n",
    "\\]\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "\\[\n",
    "z_{hidden} = [1, 0, 1] \n",
    "\\begin{bmatrix} \n",
    "0.2 & -0.3 \\\\ \n",
    "0.4 & 0.1 \\\\ \n",
    "-0.5 & 0.2 \n",
    "\\end{bmatrix}\n",
    "= [-0.3, -0.1]\n",
    "\\]\n",
    "\n",
    "Each value represents the weighted sum of inputs entering a hidden neuron **before activation**.  \n",
    "These results show that both hidden neurons receive slightly negative net inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9533b",
   "metadata": {},
   "source": [
    "### **Apply ReLU Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6b23931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply ReLU to get hidden activations (a_hidden)\n",
      "\n",
      "a_hidden = [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Apply ReLU activation to hidden units: a_hidden = max(0, z_hidden)\n",
    "print(\"Apply ReLU to get hidden activations (a_hidden)\\n\")\n",
    "a_hidden = np.maximum(0, z_hidden)\n",
    "print(\"a_hidden =\", a_hidden)   # expected [0.0, 0.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6534e",
   "metadata": {},
   "source": [
    "The **ReLU (Rectified Linear Unit)** activation function is defined as:\n",
    "\n",
    "\\[\n",
    "f(z) = \\max(0, z)\n",
    "\\]\n",
    "\n",
    "Applying it to each pre-activation value:\n",
    "\n",
    "\\[\n",
    "a_{hidden} = \\max(0, [-0.3, -0.1]) = [0, 0]\n",
    "\\]\n",
    "\n",
    "Because both pre-activation values were negative, the output becomes `0` for both hidden neurons.  \n",
    "This means neither neuron is “activated” — they both output zero to the next layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141535a1",
   "metadata": {},
   "source": [
    "### **Output Pre-activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a1d7cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output pre-activation (z_out) using θ = [bias, w_h1, w_h2]\n",
      "\n",
      "bias = -0.4, w_h1 = 0.2, w_h2 = 0.1\n",
      "z_out = -0.4\n"
     ]
    }
   ],
   "source": [
    "# Compute output pre-activation (z_out) using θ = [bias, w_h1, w_h2]\n",
    "print(\"Output pre-activation (z_out) using θ = [bias, w_h1, w_h2]\\n\")\n",
    "bias = theta[0]\n",
    "w_h1 = theta[1]\n",
    "w_h2 = theta[2]\n",
    "z_out = bias + w_h1 * a_hidden[0] + w_h2 * a_hidden[1]\n",
    "print(f\"bias = {bias}, w_h1 = {w_h1}, w_h2 = {w_h2}\")\n",
    "print(\"z_out =\", z_out)   # with a_hidden=[0,0] this equals bias (-0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccb6c2",
   "metadata": {},
   "source": [
    "Now, we calculate the **output neuron’s pre-activation value** using the output weights and bias:\n",
    "\n",
    "\\[\n",
    "z_{out} = \\theta_0 + \\theta_1 a_{h1} + \\theta_2 a_{h2}\n",
    "\\]\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "\\[\n",
    "z_{out} = (-0.4) + (0.2)(0) + (0.1)(0) = -0.4\n",
    "\\]\n",
    "\n",
    "This is the raw (unactivated) output before applying any final activation function.  \n",
    "Because all hidden activations were zero, only the bias influences the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf06243",
   "metadata": {},
   "source": [
    "### **Prediction (ŷ)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d2be7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (ŷ) using identity output activation\n",
      "\n",
      "y_hat = -0.4\n"
     ]
    }
   ],
   "source": [
    "# Prediction (ŷ) — assume identity (linear) output activation\n",
    "print(\"Prediction (ŷ) using identity output activation\\n\")\n",
    "y_hat = z_out\n",
    "print(\"y_hat =\", y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854bbab",
   "metadata": {},
   "source": [
    "In this case, the output layer uses an **identity activation function**, so the predicted value is simply:\n",
    "\n",
    "\\[\n",
    "\\hat{y} = z_{out} = -0.4\n",
    "\\]\n",
    "\n",
    "This represents the model’s final output prediction.  \n",
    "Since the target output \\( y = 1 \\), we can already expect some error between prediction and truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817a62d",
   "metadata": {},
   "source": [
    "## **Compute Error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16413d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute squared error (E = 0.5 * (y - ŷ)^2)\n",
      "\n",
      "error = 0.9799999999999999\n",
      "\n",
      "-- End --\n",
      "\n",
      "Extra (not required but informative):\n",
      " Sigmoid(output) = 0.401312339887548\n",
      " Squared error with sigmoid output = 0.17921345718546142\n",
      " Cross-entropy loss (y=1) = 0.9130152523999526\n"
     ]
    }
   ],
   "source": [
    "# Compute error (squared error E = 1/2 * (y - ŷ)^2)\n",
    "print(\"Compute squared error (E = 0.5 * (y - ŷ)^2)\\n\")\n",
    "error = 0.5 * (y - y_hat)**2\n",
    "print(\"error =\", error)   # numeric value\n",
    "print(\"\\n-- End --\\n\")\n",
    "\n",
    "# For completeness: show what would happen if we applied a sigmoid output activation\n",
    "def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "z_out_sig = z_out\n",
    "y_prob = sigmoid(z_out_sig)\n",
    "mse_sig = 0.5 * (y - y_prob)**2\n",
    "# cross-entropy loss for label y=1: -log(y_prob)\n",
    "ce_sig = -np.log(y_prob)\n",
    "\n",
    "print(\"Extra (not required but informative):\")\n",
    "print(\" Sigmoid(output) =\", y_prob)\n",
    "print(\" Squared error with sigmoid output =\", mse_sig)\n",
    "print(\" Cross-entropy loss (y=1) =\", ce_sig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78034103",
   "metadata": {},
   "source": [
    "\n",
    "We use the **Mean Squared Error (MSE)** loss function, defined as:\n",
    "\n",
    "\\[\n",
    "E = \\frac{1}{2}(y - \\hat{y})^2\n",
    "\\]\n",
    "\n",
    "Substituting values:\n",
    "\n",
    "\\[\n",
    "E = 0.5(1 - (-0.4))^2 = 0.5(1.4)^2 = 0.98\n",
    "\\]\n",
    "\n",
    "This error value (`≈ 0.98`) indicates the magnitude of difference between the model’s prediction and the true value.  \n",
    "A high error shows that the current weights are not well-optimized yet.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
