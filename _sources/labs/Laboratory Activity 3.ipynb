{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f1d183",
   "metadata": {},
   "source": [
    "# Laboratory Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84ef6a",
   "metadata": {},
   "source": [
    "![image](./images/lab3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a4c3d",
   "metadata": {},
   "source": [
    "### **Define Inputs and Weights**\n",
    "\n",
    "We begin by defining the input vector `x`, target output `y`, hidden layer weights (`W_hidden`), and output layer parameters (`theta`).  \n",
    "The learning rate (`lr`) is also set to control how much the weights are updated during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817c8243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define inputs and weights (as given)\n",
      "x = [1. 0. 1.]\n",
      "y = 1.0\n",
      "W_hidden =\n",
      " [[ 0.2 -0.3]\n",
      " [ 0.4  0.1]\n",
      " [-0.5  0.2]]\n",
      "theta = [-0.4  0.2  0.1]\n",
      "learning rate = 0.001\n"
     ]
    }
   ],
   "source": [
    "# Define inputs, weights, target, and learning rate (given values)\n",
    "import numpy as np\n",
    "\n",
    "print(\"Define inputs and weights (as given)\")\n",
    "x = np.array([1., 0., 1.])        # input vector\n",
    "y = 1.                           # target scalar\n",
    "\n",
    "W_hidden = np.array([            # shape (3,2) inputs->2 hidden units\n",
    "    [0.2, -0.3],\n",
    "    [0.4,  0.1],\n",
    "    [-0.5, 0.2]\n",
    "])\n",
    "theta = np.array([-0.4, 0.2, 0.1])  # [bias, w_h1, w_h2]\n",
    "\n",
    "lr = 0.001  # learning rate\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"W_hidden =\\n\", W_hidden)\n",
    "print(\"theta =\", theta)\n",
    "print(\"learning rate =\", lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71883a3a",
   "metadata": {},
   "source": [
    "We begin by defining the input vector `x`, target output `y`, hidden layer weights (`W_hidden`), and output layer parameters (`theta`).  \n",
    "The learning rate (`lr`) is also set to control how much the weights are updated during backpropagation.\n",
    "\n",
    "**Explanation:**\n",
    "- `x = [1, 0, 1]`: Represents one data sample with three input features.  \n",
    "- `y = 1`: The expected (target) output value.  \n",
    "- `W_hidden`: Weights connecting the input layer to the two hidden neurons.  \n",
    "- `theta`: Parameters for the output layer, including bias and weights for each hidden neuron.  \n",
    "- `lr = 0.001`: A small step size for updating the weights during learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a0bb5",
   "metadata": {},
   "source": [
    "### **Forward Pass – Hidden Pre-Activation (`z_hidden`)**\n",
    "\n",
    "This step computes the **weighted sum of inputs** for each hidden neuron before applying the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad767f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass - hidden pre-activation (z_hidden)\n",
      "z_hidden = [-0.3 -0.1]\n"
     ]
    }
   ],
   "source": [
    "# Forward pass - hidden pre-activation\n",
    "print(\"Forward pass - hidden pre-activation (z_hidden)\")\n",
    "z_hidden = x.dot(W_hidden)   # shape (2,)\n",
    "print(\"z_hidden =\", z_hidden)   # expect [-0.3, -0.1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe824c",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Formula:  \n",
    "  \\[\n",
    "  z_{hidden} = x \\times W_{hidden}\n",
    "  \\]\n",
    "- Result: `z_hidden = [-0.3, -0.1]`\n",
    "- These values represent the **raw activations** that determine whether each neuron will activate (fire) after ReLU is applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4942033",
   "metadata": {},
   "source": [
    "### **Hidden Activation using ReLU (`a_hidden`)**\n",
    "\n",
    "We now apply the **ReLU (Rectified Linear Unit)** activation function to each hidden neuron output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d14956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass - hidden activation (a_hidden) using ReLU\n",
      "a_hidden = [0. 0.]\n",
      "ReLU derivative (d a / d z) = [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Forward pass - hidden activation (ReLU)\n",
    "print(\"Forward pass - hidden activation (a_hidden) using ReLU\")\n",
    "a_hidden = np.maximum(0, z_hidden)\n",
    "relu_derivative = (z_hidden > 0).astype(float)  # derivative of ReLU wrt z_hidden\n",
    "print(\"a_hidden =\", a_hidden)\n",
    "print(\"ReLU derivative (d a / d z) =\", relu_derivative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48422161",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Formula:  \n",
    "  \\[\n",
    "  a_{hidden} = \\max(0, z_{hidden})\n",
    "  \\]\n",
    "- Result: `a_hidden = [0, 0]`\n",
    "- Since both pre-activation values are negative, ReLU outputs **0** for both neurons.\n",
    "- The **ReLU derivative** is `[0, 0]`, meaning no gradient will flow backward through these neurons — they are \"inactive.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af1455",
   "metadata": {},
   "source": [
    "### **Output Pre-Activation (`z_out`) and Prediction (`ŷ`)**\n",
    "\n",
    "We now compute the **output neuron’s pre-activation** value using the output layer parameters (`theta`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "badc9557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass - output pre-activation (z_out) and prediction y_hat\n",
      "bias = -0.4\n",
      "w_h = [0.2 0.1]\n",
      "z_out = -0.4\n",
      "y_hat = -0.4\n"
     ]
    }
   ],
   "source": [
    "# Forward pass - output pre-activation and prediction (identity)\n",
    "print(\"Forward pass - output pre-activation (z_out) and prediction y_hat\")\n",
    "bias = theta[0]\n",
    "w_h = theta[1:]   # [w_h1, w_h2]\n",
    "z_out = bias + w_h.dot(a_hidden)    # scalar\n",
    "y_hat = z_out  # identity activation\n",
    "print(\"bias =\", bias)\n",
    "print(\"w_h =\", w_h)\n",
    "print(\"z_out =\", z_out)\n",
    "print(\"y_hat =\", y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fff774",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Formula:  \n",
    "  \\[\n",
    "  z_{out} = \\text{bias} + (a_{hidden} \\cdot w_{hidden})\n",
    "  \\]\n",
    "- With `bias = -0.4`, `w_h = [0.2, 0.1]`, and `a_hidden = [0, 0]`,  \n",
    "  → `z_out = -0.4`  \n",
    "- The prediction (ŷ) uses an **identity activation**, so `ŷ = -0.4`.\n",
    "- The network predicts **-0.4**, far from the true output `y = 1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01167c",
   "metadata": {},
   "source": [
    "### **Compute Loss (Mean Squared Error)**\n",
    "\n",
    "We measure how far the prediction is from the true target using the **Mean Squared Error (MSE)** loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5294b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute loss (MSE)\n",
      "loss = 0.9799999999999999\n"
     ]
    }
   ],
   "source": [
    "# Compute loss (MSE: 0.5*(y - y_hat)^2)\n",
    "print(\"Compute loss (MSE)\")\n",
    "loss = 0.5 * (y - y_hat)**2\n",
    "print(\"loss =\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7352b0",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Formula:  \n",
    "  \\[\n",
    "  E = \\frac{1}{2}(y - \\hat{y})^2\n",
    "  \\]\n",
    "- Result: `E = 0.98`\n",
    "- A high loss indicates a large prediction error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68bc9a",
   "metadata": {},
   "source": [
    "### **Backpropagation – Gradients at Output Layer**\n",
    "\n",
    "We now compute the gradients of the loss with respect to the output neuron parameters (`theta`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef1b1755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backpropagation - gradients at output layer\n",
      "dE/dy_hat = -1.4\n",
      "dE/dz_out = -1.4\n",
      "dE/dtheta = [-1.4 -0.  -0. ]\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation - gradients at output layer\n",
    "# For identity output, d y_hat / d z_out = 1\n",
    "print(\"Backpropagation - gradients at output layer\")\n",
    "dE_dyhat = -(y - y_hat)        # derivative of 0.5*(y-ŷ)^2 wrt ŷ = -(y-ŷ)\n",
    "dyhat_dzout = 1.0\n",
    "dE_dzout = dE_dyhat * dyhat_dzout\n",
    "print(\"dE/dy_hat =\", dE_dyhat)\n",
    "print(\"dE/dz_out =\", dE_dzout)\n",
    "\n",
    "# Gradients w.r.t theta parameters: d z_out / d theta = [1, a_h1, a_h2]\n",
    "dE_dtheta = np.array([dE_dzout * 1.0, dE_dzout * a_hidden[0], dE_dzout * a_hidden[1]])\n",
    "print(\"dE/dtheta =\", dE_dtheta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdebd4fc",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- The derivative of MSE w.r.t. the output is:  \n",
    "  \\[\n",
    "  \\frac{\\partial E}{\\partial \\hat{y}} = (\\hat{y} - y)\n",
    "  \\]\n",
    "- So, `dE/dy_hat = -1.4` and `dE/dz_out = -1.4`.  \n",
    "- The gradients for output weights are then:  \n",
    "  \\[\n",
    "  dE/d\\theta = [-1.4, 0, 0]\n",
    "  \\]\n",
    "- Only the bias term receives a gradient because the hidden activations were both 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e98101",
   "metadata": {},
   "source": [
    "### **Backpropagate to Hidden Layer**\n",
    "\n",
    "Next, we compute how the error propagates back to the hidden layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e0f63ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backpropagate to hidden layer and compute dE/dW_hidden\n",
      "dE/d a_hidden = [-0.28 -0.14]\n",
      "dE/d z_hidden = [-0. -0.]\n",
      "dE/dW_hidden =\n",
      " [[-0. -0.]\n",
      " [-0. -0.]\n",
      " [-0. -0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Backpropagate to hidden layer and compute gradients for W_hidden\n",
    "print(\"Backpropagate to hidden layer and compute dE/dW_hidden\")\n",
    "\n",
    "# dE/d a_hidden = dE/d z_out * d z_out / d a_hidden = dE_dzout * w_h\n",
    "dE_dah = dE_dzout * w_h  # shape (2,)\n",
    "print(\"dE/d a_hidden =\", dE_dah)\n",
    "\n",
    "# d a / d z (ReLU derivative) computed earlier\n",
    "dE_dzh = dE_dah * relu_derivative  # shape (2,)\n",
    "print(\"dE/d z_hidden =\", dE_dzh)\n",
    "\n",
    "# d z_hidden_j / d W_hidden_ij = x_i  -> so dE/dW_hidden_ij = x_i * dE/dz_hidden_j\n",
    "# We'll compute gradient matrix same shape as W_hidden (3,2)\n",
    "dE_dW_hidden = np.zeros_like(W_hidden)\n",
    "for i in range(W_hidden.shape[0]):   # inputs\n",
    "    for j in range(W_hidden.shape[1]):  # hidden units\n",
    "        dE_dW_hidden[i, j] = x[i] * dE_dzh[j]\n",
    "\n",
    "print(\"dE/dW_hidden =\\n\", dE_dW_hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b5c19",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- The gradients passed to hidden activations:  \n",
    "  \\[\n",
    "  dE/da_{hidden} = dE/dz_{out} \\times w_h\n",
    "  \\]\n",
    "- Result: `dE/da_hidden = [-0.28, -0.14]`  \n",
    "- Because both ReLU outputs were 0, their derivatives are also 0 →  \n",
    "  \\[\n",
    "  dE/dz_{hidden} = [0, 0]\n",
    "  \\]\n",
    "- Therefore, no gradient reaches the hidden weights, and `dE/dW_hidden` is all zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb87e26",
   "metadata": {},
   "source": [
    "### **Parameter Updates**\n",
    "\n",
    "Now we update all weights and biases using the computed gradients and learning rate (`lr = 0.001`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eed55c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter updates using learning rate lr = 0.001\n",
      "theta_old = [-0.4  0.2  0.1]\n",
      "dE/dtheta = [-1.4 -0.  -0. ]\n",
      "theta_new = [-0.3986  0.2     0.1   ]\n",
      "\n",
      "W_hidden_old =\n",
      " [[ 0.2 -0.3]\n",
      " [ 0.4  0.1]\n",
      " [-0.5  0.2]]\n",
      "dE/dW_hidden =\n",
      " [[-0. -0.]\n",
      " [-0. -0.]\n",
      " [-0. -0.]]\n",
      "W_hidden_new =\n",
      " [[ 0.2 -0.3]\n",
      " [ 0.4  0.1]\n",
      " [-0.5  0.2]]\n"
     ]
    }
   ],
   "source": [
    "# Parameter updates (gradient descent)\n",
    "print(\"Parameter updates using learning rate lr =\", lr)\n",
    "\n",
    "# Update theta: theta_new = theta - lr * dE/dtheta\n",
    "theta_new = theta - lr * dE_dtheta\n",
    "# Update W_hidden similarly\n",
    "W_hidden_new = W_hidden - lr * dE_dW_hidden\n",
    "\n",
    "print(\"theta_old =\", theta)\n",
    "print(\"dE/dtheta =\", dE_dtheta)\n",
    "print(\"theta_new =\", theta_new)\n",
    "print(\"\\nW_hidden_old =\\n\", W_hidden)\n",
    "print(\"dE/dW_hidden =\\n\", dE_dW_hidden)\n",
    "print(\"W_hidden_new =\\n\", W_hidden_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec893d",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Update rule:  \n",
    "  \\[\n",
    "  \\theta_{new} = \\theta_{old} - lr \\times dE/d\\theta\n",
    "  \\]\n",
    "- Only the bias term changes slightly:  \n",
    "  `theta_new = [-0.3986, 0.2, 0.1]`  \n",
    "- Hidden weights remain the same because their gradients were zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61a40a",
   "metadata": {},
   "source": [
    "### **Summary and Interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3bc8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary and interpretation\n",
      "Because both hidden ReLU units were inactive (a_hidden = [0,0]), the gradients flowing\n",
      "into the hidden weights are zero for inputs where x=0, and proportional to x where x=1.\n",
      "Numeric results:\n",
      "dE/dtheta: [-1.4 -0.  -0. ]\n",
      "dE/dW_hidden:\n",
      " [[-0. -0.]\n",
      " [-0. -0.]\n",
      " [-0. -0.]]\n",
      "\n",
      "Note: Because a_hidden = [0,0], dE/dtheta's second and third components are zero.\n",
      "Also, the output error is large because prediction (-0.4) is far from target (1).\n",
      "-- End --\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary of gradients and small commentary\n",
    "print(\"Summary and interpretation\")\n",
    "print(f\"Because both hidden ReLU units were inactive (a_hidden = [0,0]), the gradients flowing\")\n",
    "print(\"into the hidden weights are zero for inputs where x=0, and proportional to x where x=1.\")\n",
    "print(\"Numeric results:\")\n",
    "print(\"dE/dtheta:\", dE_dtheta)\n",
    "print(\"dE/dW_hidden:\\n\", dE_dW_hidden)\n",
    "print(\"\\nNote: Because a_hidden = [0,0], dE/dtheta's second and third components are zero.\")\n",
    "print(\"Also, the output error is large because prediction (-0.4) is far from target (1).\")\n",
    "print(\"-- End --\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc1611",
   "metadata": {},
   "source": [
    "**Final Analysis:**\n",
    "- Both hidden ReLU units were inactive (`a_hidden = [0, 0]`), so they did not contribute to learning in this step.  \n",
    "- As a result, the hidden weights did **not update**, and only the bias term changed slightly.\n",
    "- The prediction error remains high (`ŷ = -0.4` vs `y = 1`), showing that the network did not learn effectively on this input.\n",
    "\n",
    "**Key takeaways:**\n",
    "- ReLU can \"die\" (stop learning) if neurons receive only negative activations.  \n",
    "- Proper initialization and varied inputs are crucial for training success.  \n",
    "- Forward and backward propagation form the foundation of neural network learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa36b5c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
